{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cafbea-e4aa-446d-b1ae-7d4ec923db32",
   "metadata": {},
   "source": [
    "## Handle Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5f6a95-55ac-407d-a52f-ee598e898c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "import shutil\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from chromedriver_py import binary_path\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a2520-b0b3-4b66-92bd-d3a89ae8a790",
   "metadata": {},
   "source": [
    "## Create Greenwashing Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab8434c3-3fc1-44f4-986d-56bbbdcc54c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tothb\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Predefined question or query\n",
    "predefined_definition = \"Greenwashing is the deceptive act of employing behaviors, activities, or communication strategies that deliberately create an inflated perception of a company's environmental commitment and performance. This manipulation can involve unsubstantiated claims of sustainability, misleading marketing tactics, selective disclosure of environmental data, implementation of superficial eco-friendly practices, and misrepresentation of a product's environmental impact.\"\n",
    "\n",
    "# Four predefined definitions\n",
    "\n",
    "DESC_Deception_and_Misinformation = 'This is when companies engage in deceptive practices by advertising products as \"eco-friendly\" or \"sustainable\" without providing supporting evidence. By doing so, they mislead consumers and in the longer run also compromise genuine sustainability efforts. Their claims involve a range of misleading actions, including misrepresenting product ingredients or origins.'\n",
    "DESC_Misleading_Communication = 'Companies highlight trivial environmental initiatives, such as recycling programs or energy-efficient manufacturing, but at the same time, they downplay or ignore significant environmental impacts like emissions and pollution. This is a facade of environmental responsibility which misleads stakeholders about the companys actual ecological footprint. The strategy is misleading by disproportion rather than outright fabrication.'\n",
    "DESC_Selective_Disclosure = 'Companies selectively disclose positive environmental activities, cherry-picking data to present an overly positive view of their environmental impact. An example to this is highlighting achievements like participation in renewable energy initiatives while omitting information about activities that have a significant negative environmental impact (such as high levels of waste production or biodiversity loss). Companies shift the narrative and mislead stakeholders by omission by presenting a distorted image that hides the actual environmental cost of their operations.'\n",
    "DESC_Greenwashing_as_Decoupling = 'Companies create a mask of being sustainable through branding and symbolic actions that are decoupled from their actual environmental impact. These strategies often involve initiatives that - while visually appealing or publicly engaging -, do not result in substantial improvements to the companys environmental performance. Examples include high-profile but limited scope sustainability campaigns or using \"green\" imagery in marketing materials. This misleads consumers by creating a misleading impression of environmental responsibility.'\n",
    "\n",
    "# Initialize the Sentence-BERT model\n",
    "model = SentenceTransformer('all-miniLM-L6-v2') # Or the lightweight all-miniLM-L6-v2\n",
    "\n",
    "# Encode the predefined question to get its embedding\n",
    "definition_embedding = model.encode(predefined_definition)\n",
    "\n",
    "QUERY_Deception_and_Misinformation = model.encode(DESC_Deception_and_Misinformation)\n",
    "QUERY_Misleading_Communication = model.encode(DESC_Misleading_Communication)\n",
    "QUERY_Selective_Disclosure = model.encode(DESC_Selective_Disclosure)\n",
    "QUERY_Greenwashing_as_Decoupling = model.encode(DESC_Greenwashing_as_Decoupling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d67d0-0b73-43be-a853-f922fc8384be",
   "metadata": {},
   "source": [
    "## Get Data From Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59eb1b4d-4f17-4a2d-938c-f1fd41c28d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def open_webpage_and_search(file_name):\n",
    "    \n",
    "    # Intitialize webscraper\n",
    "    service_object = Service(binary_path)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Uncomment this line if you don't want the browser window to open\n",
    "    driver = webdriver.Chrome(service=service_object, options=options)\n",
    "    \n",
    "    # Open the homepage of Google News\n",
    "    google_news_page = \"https://news.google.com/home?hl=en-US&gl=US&ceid=US:en\"\n",
    "    driver.get(google_news_page)\n",
    "    time.sleep(3)  # It's better to use WebDriverWait here as well for a more reliable wait\n",
    "    \n",
    "    # Wait for the element to be clickable and click it\n",
    "    # Assuming this click is necessary before searching. If not, adjust accordingly.\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[2]/div/div/button/span'))\n",
    "    ).click()\n",
    "    \n",
    "    # Wait for the input field to be clickable\n",
    "    input_field = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \".Ax4B8.ZAGvjd\"))\n",
    "    )\n",
    "    \n",
    "    # Send text to the input field and press ENTER\n",
    "    input_field.send_keys(file_name + Keys.ENTER)\n",
    "\n",
    "    time.sleep(3)  # Let the search results page load. Consider using WebDriverWait here too.\n",
    "\n",
    "    # Return the current URL after performing the search\n",
    "    current_url = driver.current_url\n",
    "    driver.quit()  # Close the browser after fetching the URL\n",
    "    return current_url\n",
    "\n",
    "\n",
    "def get_the_data_searched(file_name, url):\n",
    "    \n",
    "    # Intitialize webscraper\n",
    "    service_object = Service(binary_path)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Uncomment this line if you don't want the browser window to open\n",
    "    driver = webdriver.Chrome(service=service_object, options=options)\n",
    "    \n",
    "    # Open the homepage of Google News\n",
    "    url_search = url\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Assuming this click is necessary before searching. If not, adjust accordingly.\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[2]/div/div/button/span'))\n",
    "    ).click()\n",
    "    \n",
    "    time.sleep(3)  # It's better to use WebDriverWait here as well for a more reliable wait\n",
    "\n",
    "    # Collection of articles data including dates\n",
    "    articles = []\n",
    "    title_elements = driver.find_elements(By.CLASS_NAME, 'JtKRv')\n",
    "    date_elements = driver.find_elements(By.CLASS_NAME, 'hvbAAd') # Assuming this class name is correct for dates\n",
    "    \n",
    "    if len(title_elements) != 0:\n",
    "\n",
    "        if len(title_elements) == len(date_elements):  # Ensuring each title has a corresponding date\n",
    "            for title_element, date_element in zip(title_elements, date_elements):\n",
    "                link = title_element.get_attribute('href')\n",
    "                title = title_element.text\n",
    "                date = date_element.get_attribute('datetime')\n",
    "                article_data = {\n",
    "                    'Title': title,\n",
    "                    'Original Link': link,\n",
    "                    'Date': date,\n",
    "                }\n",
    "                articles.append(article_data)\n",
    "        else:\n",
    "            print(\"Mismatch in the number of titles and dates found.\")\n",
    "            print(f\"Titles found: {len(title_elements)}\")\n",
    "            print(f\"Dates found: {len(date_elements)}\")\n",
    "            print(f\"Links found: {len(link_elements)}\")\n",
    "\n",
    "        # Convert initial collection into a DataFrame\n",
    "        articles_df = pd.DataFrame(articles)\n",
    "\n",
    "        # print(articles_df)\n",
    "        print(len(articles_df['Title']))\n",
    "\n",
    "\n",
    "        # Now iterate through each original link to get the final URL and site name\n",
    "        # for index, row in articles_df.iterrows():\n",
    "            # original_link = row['Original Link']\n",
    "            # driver.get(original_link)\n",
    "            # time.sleep(3)  # Allow time for any redirects and for the site to load\n",
    "            # final_url = driver.current_url\n",
    "            # parsed_url = urlparse(final_url)\n",
    "            # site_name = parsed_url.netloc.replace('www.', '')  # Clean the site name\n",
    "            # articles_df.at[index, 'Final Link'] = final_url\n",
    "            # articles_df.at[index, 'Site Name'] = site_name\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        # Now iterate through the titles in your DataFrame to calculate similarity scores\n",
    "        similarity_scores_general = []\n",
    "        \n",
    "        similarity_scores1 = []\n",
    "        similarity_scores2 = []\n",
    "        similarity_scores3 = []\n",
    "        similarity_scores4 = []\n",
    "        \n",
    "        for title in articles_df['Title']:\n",
    "            title_embedding = model.encode(title)\n",
    "            # Calculate cosine similarity\n",
    "            similarity_score = util.pytorch_cos_sim(definition_embedding, title_embedding)\n",
    "            similarity_scores_general.append(similarity_score.item())\n",
    "            \n",
    "            # NEWNEWNEW\n",
    "            similarity_score1= util.pytorch_cos_sim(QUERY_Deception_and_Misinformation, title_embedding)\n",
    "            similarity_scores1.append(similarity_score1.item())\n",
    "            \n",
    "            similarity_score2= util.pytorch_cos_sim(QUERY_Misleading_Communication, title_embedding)\n",
    "            similarity_scores2.append(similarity_score2.item())\n",
    "            \n",
    "            similarity_score3= util.pytorch_cos_sim(QUERY_Selective_Disclosure, title_embedding)\n",
    "            similarity_scores3.append(similarity_score3.item())\n",
    "            \n",
    "            similarity_score4= util.pytorch_cos_sim(QUERY_Greenwashing_as_Decoupling, title_embedding)\n",
    "            similarity_scores4.append(similarity_score4.item())\n",
    "            \n",
    "        \n",
    "        # NEWNEWNEW\n",
    "        articles_df['General_Definition'] = similarity_scores_general\n",
    "        articles_df['Deception_and_Misinformation'] = similarity_scores1\n",
    "        articles_df['Misleading_Communication'] = similarity_scores2\n",
    "        articles_df['Selective_Disclosure'] = similarity_scores3\n",
    "        articles_df['Greenwashing_as_Decoupling'] = similarity_scores4\n",
    "\n",
    "        # Sort the DataFrame based on similarity scores in descending order to see the most relevant articles first\n",
    "        # articles_df = articles_df.sort_values(by='Similarity Score', ascending=False)\n",
    "    else:\n",
    "        article_data = {\n",
    "                    'Title': \"No search results\",\n",
    "                    'Original Link': \"No search results\",\n",
    "                    'Date': \"No search results\",\n",
    "                }\n",
    "        \n",
    "        articles.append(article_data)\n",
    "        \n",
    "        articles_df = pd.DataFrame(articles)\n",
    "        \n",
    "        print(len(articles_df['Title']))\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    # print(articles_df)\n",
    "    \n",
    "    return articles_df\n",
    "\n",
    "def save_to_excel_file(file_name, articles_df):\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Ensure the file_name variable ends with '.xlsx'\n",
    "    file_name_final = file_name + \"_\" + current_date +\".xlsx\"  # Example file name, adjusted to include .xlsx\n",
    "\n",
    "    # Attempt to save the DataFrame to an Excel file again\n",
    "    try:\n",
    "        articles_df.to_excel(file_name_final, index=False)\n",
    "        print(f\"Data saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save data to Excel file: {e}\")\n",
    "    \n",
    "\n",
    "def do_a_search_to_excel(search_term):\n",
    "    \n",
    "    url_for_page = open_webpage_and_search(search_term)\n",
    "    \n",
    "    articles_df = get_the_data_searched(search_term, url_for_page)\n",
    "    \n",
    "    save_to_excel_file(search_term, articles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9acf6f-68bf-4b65-a4ae-fd7c7ef098db",
   "metadata": {},
   "source": [
    "## Do Scoring on Retrieved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3770fb3d-5b72-4d45-9c4c-de959bb3a247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_companies_news(company_names):\n",
    "    # Initialize DataFrame to hold the final results\n",
    "    # results_df = pd.DataFrame(columns=['Company Name', 'Average Similarity Score', 'Number of Articles', 'Ranking'])\n",
    "    \n",
    "    # NEWNEWNEW\n",
    "    \n",
    "    results_df = pd.DataFrame(columns=['Company Name', \n",
    "                                       'Average General_Definition Score',\n",
    "                                       'Average Deception_and_Misinformation Score',\n",
    "                                       'Average Misleading_Communication Score',\n",
    "                                       'Average Selective_Disclosure Score',\n",
    "                                       'Average Greenwashing_as_Decoupling Score',\n",
    "                                       'Number of Articles',\n",
    "                                       'General_Definition Ranking',\n",
    "                                       'Deception_and_Misinformation Ranking',\n",
    "                                       'Misleading_Communication Ranking',\n",
    "                                       'Selective_Disclosure Ranking',\n",
    "                                       'Greenwashing_as_Decoupling Ranking'])\n",
    "    \n",
    "    for company_name in company_names:\n",
    "        url_for_page = open_webpage_and_search(company_name)\n",
    "        articles_df = get_the_data_searched(company_name, url_for_page)\n",
    "        \n",
    "        # Check if articles_df is not empty or if a specific condition for no results is met\n",
    "        if not articles_df.empty and not (articles_df.iloc[0]['Title'] == \"No search results\"):\n",
    "            # average_similarity_score = articles_df['Similarity Score'].mean() if 'Similarity Score' in articles_df.columns else 0\n",
    "            \n",
    "            # NEWNEWNEW\n",
    "            average_similarity_score_all = articles_df['General_Definition'].mean() if 'General_Definition' in articles_df.columns else 0\n",
    "            average_similarity_score1 = articles_df['Deception_and_Misinformation'].mean() if 'Deception_and_Misinformation' in articles_df.columns else 0\n",
    "            average_similarity_score2 = articles_df['Misleading_Communication'].mean() if 'Misleading_Communication' in articles_df.columns else 0\n",
    "            average_similarity_score3 = articles_df['Selective_Disclosure'].mean() if 'Selective_Disclosure' in articles_df.columns else 0\n",
    "            average_similarity_score4 = articles_df['Greenwashing_as_Decoupling'].mean() if 'Greenwashing_as_Decoupling' in articles_df.columns else 0\n",
    "            \n",
    "            \n",
    "            print(average_similarity_score1)\n",
    "            print(average_similarity_score2)\n",
    "            print(average_similarity_score3)\n",
    "            print(average_similarity_score4)\n",
    "            \n",
    "            number_of_articles = len(articles_df)\n",
    "            ranking_all = (average_similarity_score_all * number_of_articles)\n",
    "            ranking1 = (average_similarity_score1 * number_of_articles)\n",
    "            ranking2 = (average_similarity_score2 * number_of_articles)\n",
    "            ranking3 = (average_similarity_score3 * number_of_articles)\n",
    "            ranking4 = (average_similarity_score4 * number_of_articles)\n",
    "            result = {'Company Name': company_name, \n",
    "                      'Average General_Definition Score': average_similarity_score_all,\n",
    "                      'Average Deception_and_Misinformation Score': average_similarity_score1, \n",
    "                      'Average Misleading_Communication Score': average_similarity_score2, \n",
    "                      'Average Selective_Disclosure Score': average_similarity_score3, \n",
    "                      'Average Greenwashing_as_Decoupling Score': average_similarity_score4, \n",
    "                      'Number of Articles': number_of_articles,\n",
    "                      'General_Definition Ranking': ranking_all,\n",
    "                      'Deception_and_Misinformation Ranking': ranking1,\n",
    "                      'Misleading_Communication Ranking': ranking2,\n",
    "                      'Selective_Disclosure Ranking': ranking3,\n",
    "                      'Greenwashing_as_Decoupling Ranking': ranking4}\n",
    "        else:\n",
    "            # Handle case with no articles found\n",
    "            print(\"No Score\")\n",
    "            result = {'Company Name': company_name, \n",
    "                      'Average General_Definition Score': 0,\n",
    "                      'Average Deception_and_Misinformation Score': 0, \n",
    "                      'Average Misleading_Communication Score': 0, \n",
    "                      'Average Selective_Disclosure Score': 0, \n",
    "                      'Average Greenwashing_as_Decoupling Score': 0, \n",
    "                      'Number of Articles': 0,\n",
    "                      'General_Definition Ranking': 0,\n",
    "                      'Deception_and_Misinformation Ranking': 0,\n",
    "                      'Misleading_Communication Ranking': 0,\n",
    "                      'Selective_Disclosure Ranking': 0,\n",
    "                      'Greenwashing_as_Decoupling Ranking': 0}\n",
    "        \n",
    "        # Append the result to results_df\n",
    "        results_df = results_df._append(result, ignore_index=True)\n",
    "    \n",
    "    results_df = results_df.sort_values(by='Deception_and_Misinformation Ranking', ascending=False)\n",
    "    \n",
    "    # Save results to an Excel file\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "    random_number = random.randint(10000, 99999)\n",
    "    filename = f\"company_news_analysis_{current_date}_{random_number}.xlsx\"\n",
    "    results_df.to_excel(filename, index=False)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692db12-7d4d-433c-bb85-62c3122128ae",
   "metadata": {},
   "source": [
    "## List of Firms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c243c7d-e35e-45e5-a0ca-c6d36ca67d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of the names of the firms you want to score\n",
    "company_names = [\n",
    "    \"Company 1\",\n",
    "    \"Company 2\",\n",
    "    \"Company 3\",\n",
    "    \"Etc.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ed723-d3b5-4efd-88fb-114d33537b31",
   "metadata": {},
   "source": [
    "## Run The Program with Cocurrent Threading Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1628e264-b67d-4c93-8b8e-e6d6e899ca27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to company_news_analysis_20240622_36434.xlsx\n",
      "File 'company_news_analysis_20240622_36434.xlsx' moved to folder 'greenwashing_run_collection_20240622_QoHXwl'.\n",
      "Batch completed\n",
      "78\n",
      "99\n",
      "64\n",
      "77\n",
      "0.3335249175628026\n",
      "0.3919228794865119\n",
      "0.3366131643072153\n",
      "0.39439234118431044\n",
      "Results saved to company_news_analysis_20240622_42340.xlsx\n",
      "File 'company_news_analysis_20240622_42340.xlsx' moved to folder 'greenwashing_run_collection_20240622_QoHXwl'.\n",
      "Batch completed\n",
      "0.3646273992090213\n",
      "0.42014908722855826\n",
      "0.359499031357994\n",
      "0.4400491548909081\n",
      "Results saved to company_news_analysis_20240622_94343.xlsx\n",
      "File 'company_news_analysis_20240622_94343.xlsx' moved to folder 'greenwashing_run_collection_20240622_QoHXwl'.\n",
      "Batch completed\n",
      "0.3000341334118275\n",
      "0.33397615172725637\n",
      "0.29700204369146377\n",
      "0.3831297548022121\n",
      "Results saved to company_news_analysis_20240622_49327.xlsx\n",
      "File 'company_news_analysis_20240622_49327.xlsx' moved to folder 'greenwashing_run_collection_20240622_QoHXwl'.\n",
      "Batch completed\n",
      "0.3183832826552453\n",
      "0.3845911448845616\n",
      "0.3312822620977055\n",
      "0.39725049672188695\n",
      "Results saved to company_news_analysis_20240622_20617.xlsx\n",
      "File 'company_news_analysis_20240622_20617.xlsx' moved to folder 'greenwashing_run_collection_20240622_QoHXwl'.\n",
      "Batch completed\n",
      "All Excel files have been merged into 'greenwashing_run_collection_20240622_QoHXwl\\merged_excel_files.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "# Function to divide the list of company names into batches\n",
    "def divide_list_into_batches(lst, n):\n",
    "    \"\"\"Divides a list into n nearly equal parts.\"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "# Functions to generate and create a new folder for saving files\n",
    "def generate_folder_name(base=\"greenwashing_run_collection\"):\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "    random_chars = ''.join(random.choices(string.ascii_letters + string.digits, k=6))\n",
    "    folder_name = f\"{base}_{current_date}_{random_chars}\"\n",
    "    return folder_name\n",
    "\n",
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    return folder_name\n",
    "\n",
    "# Wrapper function to analyze news and move the output file to the specified folder\n",
    "def analyze_companies_news_wrapper(search_terms, folder_name):\n",
    "    filename = analyze_companies_news(search_terms)\n",
    "    current_file_path = os.path.join(os.getcwd(), filename)\n",
    "    target_file_path = os.path.join(folder_name, filename)\n",
    "    if os.path.exists(current_file_path):\n",
    "        shutil.move(current_file_path, target_file_path)\n",
    "        print(f\"File '{filename}' moved to folder '{folder_name}'.\")\n",
    "    else:\n",
    "        print(f\"Error: File '{filename}' not found in the current directory.\")\n",
    "\n",
    "# Generate and create the folder\n",
    "folder_name = generate_folder_name()\n",
    "create_folder(folder_name)\n",
    "\n",
    "# Desired number of batches and threads\n",
    "num_batches = 5  # Adjust as per your requirement\n",
    "num_threads = min(num_batches, 5)  # Adjust based on system capabilities\n",
    "\n",
    "# Generate batches from the list of company names\n",
    "batches = divide_list_into_batches(company_names, num_batches)\n",
    "\n",
    "# Convert company names in each batch to search terms\n",
    "all_batches = [[f\"{company} greenwashing\" for company in batch] for batch in batches]\n",
    "\n",
    "# Using ThreadPoolExecutor to run tasks concurrently\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(analyze_companies_news_wrapper, batch, folder_name) for batch in all_batches]\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as exc:\n",
    "            print(f'Batch generated an exception: {exc}')\n",
    "        else:\n",
    "            print('Batch completed')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def merge_excel_files(folder_name, output_file_name):\n",
    "    # List all Excel files in the folder\n",
    "    excel_files = [f for f in os.listdir(folder_name) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
    "    \n",
    "    # Initialize an empty DataFrame to hold all the data\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through the Excel files and append them to the merged_df DataFrame\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(folder_name, file)\n",
    "        df = pd.read_excel(file_path)\n",
    "        merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "    \n",
    "    # Save the merged DataFrame to a new Excel file\n",
    "    output_path = os.path.join(folder_name, output_file_name)\n",
    "    merged_df.to_excel(output_path, index=False)\n",
    "    print(f\"All Excel files have been merged into '{output_path}'.\")\n",
    "\n",
    "# After the ThreadPoolExecutor block completes, call the merge_excel_files function\n",
    "output_file_name = \"merged_excel_files.xlsx\"  # You can customize the output file name\n",
    "merge_excel_files(folder_name, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a5c77-3ee1-403b-a164-f63c81ca1694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
